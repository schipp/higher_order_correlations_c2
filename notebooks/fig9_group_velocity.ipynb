{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bafacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import dask.array as da\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pywt\n",
    "import torch\n",
    "from cmcrameri import cm\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_velocity = 1.9\n",
    "length_of_oneside = 120\n",
    "# 10 Hz for smooth velocity measurements\n",
    "sampling_rate = 10\n",
    "\n",
    "# place receiver stations\n",
    "# increase grid_spacing if you can't run the notebook due to system constraints.\n",
    "grid_spacing = 0.5\n",
    "x_coords = torch.arange(-35, 35 + grid_spacing, grid_spacing)\n",
    "y_coords = torch.arange(-35, 35 + grid_spacing, grid_spacing)\n",
    "array_stations = grid_points = torch.cartesian_prod(x_coords, y_coords).float()\n",
    "\n",
    "# place auxiliary stations\n",
    "auxiliary_radius = 75\n",
    "n_aux = 180\n",
    "auxiliary_angles = np.linspace(0, 2 * np.pi, n_aux, endpoint=False)\n",
    "aux_stations = torch.tensor(\n",
    "    np.stack(\n",
    "        [\n",
    "            auxiliary_radius * np.cos(auxiliary_angles),\n",
    "            auxiliary_radius * np.sin(auxiliary_angles),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    ").float()\n",
    "\n",
    "# define sources\n",
    "n_boundary_sources = 180\n",
    "boundary_source_radius = 100\n",
    "n_cluster_sources = 6\n",
    "cluster_spread = 0\n",
    "\n",
    "boundary_source_angles = np.arange(0, 2 * np.pi, 2 * np.pi / n_boundary_sources)\n",
    "boundary_sources = torch.tensor(\n",
    "    np.stack(\n",
    "        [\n",
    "            boundary_source_radius * np.cos(boundary_source_angles),\n",
    "            boundary_source_radius * np.sin(boundary_source_angles),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "angle = 0.8 * np.pi\n",
    "x_center, y_center = (\n",
    "    boundary_source_radius * np.cos(angle),\n",
    "    boundary_source_radius * np.sin(angle),\n",
    ")\n",
    "\n",
    "cluster_sources = (\n",
    "    torch.rand(n_cluster_sources, 2) * cluster_spread - cluster_spread / 2\n",
    ") + torch.tensor([x_center, y_center])\n",
    "\n",
    "sources = torch.cat([boundary_sources, cluster_sources], dim=0).float()\n",
    "\n",
    "# compute distances between sources and stations\n",
    "distances_array = torch.cdist(sources, array_stations)\n",
    "distances_aux = torch.cdist(sources, aux_stations)\n",
    "travel_times_array = distances_array / medium_velocity\n",
    "travel_times_aux = distances_aux / medium_velocity\n",
    "\n",
    "# define time\n",
    "times = torch.arange(0, 2 * length_of_oneside + 1 / sampling_rate, 1 / sampling_rate)\n",
    "freqs = torch.fft.fftfreq(times.shape[0], d=1 / sampling_rate)\n",
    "omega = 2 * np.pi * freqs\n",
    "lapse_times_c1 = torch.arange(\n",
    "    -length_of_oneside, length_of_oneside + 1 / sampling_rate, 1 / sampling_rate\n",
    ")\n",
    "\n",
    "# define wavelet to have same length as times and wavelet in center\n",
    "# same length required for fft-convolution\n",
    "wavelet_length = 12 * sampling_rate\n",
    "wavelet_long = torch.zeros_like(times)\n",
    "wavelet = pywt.ContinuousWavelet(\"mexh\")\n",
    "wavelet_time = torch.arange(-wavelet_length // 2, wavelet_length // 2)\n",
    "wavelet, _ = wavelet.wavefun(length=wavelet_length)\n",
    "wavelet = torch.tensor(wavelet)\n",
    "# expand wavelet to length of travel times, keep it centered\n",
    "wavelet_long[\n",
    "    wavelet_long.shape[0] // 2 - wavelet_length // 2 : wavelet_long.shape[0] // 2\n",
    "    + wavelet_length // 2\n",
    "] = wavelet\n",
    "\n",
    "wavelet_long_freqs = torch.fft.fftfreq(\n",
    "    2 * length_of_oneside * sampling_rate + 1, d=1 / sampling_rate\n",
    ")\n",
    "wavelet_long_spectrum = torch.fft.fft(torch.fft.fftshift(wavelet_long))\n",
    "\n",
    "# visualise final wavelet\n",
    "fig, axs = plt.subplots(2)\n",
    "axs[0].plot(times, wavelet_long)\n",
    "axs[0].set_xlabel(\"Time (s)\")\n",
    "axs[1].plot(\n",
    "    torch.fft.fftshift(wavelet_long_freqs),\n",
    "    torch.fft.fftshift(torch.abs(wavelet_long_spectrum)),\n",
    ")\n",
    "axs[1].set_xlim(0, 1)\n",
    "axs[1].set_xlabel(\"Frequency (Hz)\")\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(*array_stations.T, s=1, label=\"Array Stations\")\n",
    "ax.scatter(*sources.T, s=1, label=\"Sources\", color=\"C3\")\n",
    "ax.scatter(*aux_stations.T, s=1, label=\"Auxiliary Stations\", color=\"C4\")\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move stuff to dask\n",
    "omega_da = da.from_array(omega.numpy(), chunks=-1)\n",
    "travel_times_master = travel_times_array[\n",
    "    :, np.argmin(np.linalg.norm(array_stations, axis=1))\n",
    "]\n",
    "travel_times_master_da = da.from_array(travel_times_master.numpy(), chunks=25)\n",
    "travel_times_array_da = da.from_array(travel_times_array.numpy(), chunks=25)\n",
    "travel_times_aux_da = da.from_array(travel_times_aux.numpy(), chunks=25)\n",
    "wavelet_long_freq_da = da.from_array(wavelet_long_spectrum.numpy(), chunks=-1)\n",
    "\n",
    "master_idx = np.argmin(np.linalg.norm(array_stations, axis=1))\n",
    "\n",
    "# go\n",
    "greens_master_da = (\n",
    "    da.exp(-1j * omega_da[None, :] * travel_times_master_da[:, None])\n",
    "    * wavelet_long_freq_da[None, :]\n",
    ")\n",
    "greens_array_da = (\n",
    "    da.exp(-1j * omega_da[None, None, :] * travel_times_array_da[:, :, None])\n",
    "    * wavelet_long_freq_da[None, None, :]\n",
    ")\n",
    "greens_aux_da = (\n",
    "    da.exp(-1j * omega_da[None, None, :] * travel_times_aux_da[:, :, None])\n",
    "    * wavelet_long_freq_da[None, None, :]\n",
    ")\n",
    "\n",
    "covariances_for_c2 = da.einsum(\n",
    "    \"srw,saw->raw\",\n",
    "    greens_array_da,\n",
    "    greens_aux_da.conj(),\n",
    ")\n",
    "# last dimension chunk must be -1 for ifft\n",
    "# the other numers are tuned to my system\n",
    "correlations_for_c2 = da.fft.fftshift(\n",
    "    da.fft.ifft(covariances_for_c2, axis=-1).real, axes=-1\n",
    ")\n",
    "\n",
    "covariances_for_c1 = da.einsum(\n",
    "    \"srw,sw->rw\",\n",
    "    greens_array_da,\n",
    "    greens_array_da[:, master_idx, :].conj(),\n",
    ")\n",
    "# last dimension chunk must be -1 for ifft\n",
    "# the other numers are tuned to my system\n",
    "correlations_for_c1 = da.fft.fftshift(\n",
    "    da.fft.ifft(covariances_for_c1, axis=-1).real, axes=-1\n",
    ")\n",
    "\n",
    "\n",
    "correlations_for_c2 = torch.tensor(correlations_for_c2.compute())\n",
    "correlations_for_c1 = torch.tensor(correlations_for_c1.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREP FOR C2\n",
    "lapse_times_c1 = torch.arange(\n",
    "    -length_of_oneside, length_of_oneside + 1 / sampling_rate, 1 / sampling_rate\n",
    ")\n",
    "c1_idx0 = torch.argmin(lapse_times_c1.abs())\n",
    "window_length = length_of_oneside\n",
    "\n",
    "start_idxs = torch.tensor([c1_idx0 - (window_length * sampling_rate), c1_idx0])\n",
    "end_idxs = start_idxs + int(window_length * sampling_rate + 1)\n",
    "\n",
    "# init empty tensor with dimensions (n_windows, n_array, n_aux, n_samples_in_window)\n",
    "correlations_for_c2_master_windows = torch.empty(\n",
    "    len(start_idxs),\n",
    "    correlations_for_c2.shape[1],\n",
    "    window_length * sampling_rate + 1,\n",
    ")\n",
    "\n",
    "# same for array stations\n",
    "correlations_for_c2_array_windows = torch.empty(\n",
    "    len(start_idxs),\n",
    "    *correlations_for_c2.shape[:-1],\n",
    "    window_length * sampling_rate + 1,\n",
    ")\n",
    "\n",
    "for win_idx, (start_idx, end_idx) in enumerate(zip(start_idxs, end_idxs)):\n",
    "    # print(win_idx, start_idx, end_idx)\n",
    "    correlations_for_c2_master_windows[win_idx] = correlations_for_c2[\n",
    "        master_idx, :, start_idx:end_idx\n",
    "    ]\n",
    "\n",
    "    correlations_for_c2_array_windows[win_idx] = correlations_for_c2[\n",
    "        :, :, start_idx:end_idx\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f210b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2\n",
    "correlations_for_c2_master_windows_da = da.from_array(\n",
    "    correlations_for_c2_master_windows.float().numpy(), chunks=[25, 25, -1]\n",
    ")\n",
    "correlations_for_c2_array_windows_da = da.from_array(\n",
    "    correlations_for_c2_array_windows.float().numpy(),\n",
    "    chunks=[25, 25, 25, -1],\n",
    ")\n",
    "\n",
    "# t: number of time windows (causal + anti-causal)\n",
    "# r: number of receiver stations\n",
    "# a: number of auxiliary stations\n",
    "# w: number of samples in window\n",
    "c2_covariances_unstacked = da.einsum(\n",
    "    \"taw,traw->traw\",\n",
    "    da.fft.fft(correlations_for_c2_master_windows_da, axis=-1).conj(),\n",
    "    da.fft.fft(correlations_for_c2_array_windows_da, axis=-1),\n",
    ")\n",
    "c2_correlations_unstacked = da.fft.fftshift(\n",
    "    da.fft.ifft(c2_covariances_unstacked, axis=-1).real,\n",
    "    axes=-1,\n",
    ")\n",
    "c2_correlations_unstacked = torch.tensor(c2_correlations_unstacked.compute())\n",
    "\n",
    "c2_correlations_unstacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter broadband around 0.3\n",
    "fmin, fmax = 0.3 / 2, 0.3 * 2\n",
    "taper = torch.hann_window(window_length * sampling_rate + 1)\n",
    "b, a = butter(4, [fmin, fmax], btype=\"band\", fs=sampling_rate)\n",
    "\n",
    "# acausal filtering\n",
    "c1_correlations_filt = torch.tensor(filtfilt(b, a, correlations_for_c1, axis=-1).copy())\n",
    "c2_correlations_unstacked_filt = torch.tensor(\n",
    "    filtfilt(b, a, c2_correlations_unstacked, axis=-1).copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average causal and anti-causal parts\n",
    "c2_correlations_unstacked_filt_avg = c2_correlations_unstacked_filt.mean(dim=0)\n",
    "\n",
    "# all-direction selection.\n",
    "c2_correlations_filt = c2_correlations_unstacked_filt_avg.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90dfe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead, do endfire-lobe based selection\n",
    "# select endfire-lobe style\n",
    "\n",
    "\n",
    "# Roux et al. 2004\n",
    "\n",
    "f = 0.3\n",
    "omega = 2 * torch.pi * f\n",
    "deltaomega = omega * 0.1\n",
    "c = medium_velocity\n",
    "\n",
    "results = []\n",
    "# stations_all = cart_coords[station_idxs_array]\n",
    "stations_all = array_stations\n",
    "# station_A = cart_coords[master_idx][0]  # master station\n",
    "station_A = torch.tensor([0.0, 0.0], dtype=torch.float32)  # master station\n",
    "stations_aux = aux_stations\n",
    "masks_endfire = []\n",
    "for station_B in stations_all:\n",
    "    # distance_from_center is half distance between the two stations\n",
    "    a = distance_from_center = torch.norm(station_A - station_B) / 2\n",
    "    stations = torch.tensor([[-a, 0], [a, 0]], dtype=torch.float32)\n",
    "    R = 2 * a\n",
    "\n",
    "    # measure the angle between the two stations\n",
    "    d_a_b = torch.norm(stations[0] - stations[1])\n",
    "    dtheta = torch.linspace(-torch.pi / 2, torch.pi / 2, 721)\n",
    "\n",
    "    # \"directivity pattern\"\n",
    "    B = 1 - dtheta**4 / 8 * (R / c) ** 2 * (omega**2 + deltaomega**2 / 12)\n",
    "\n",
    "    # remove dthetas corresponding to B < 0\n",
    "    dtheta = dtheta[B >= 0]\n",
    "    B = B[B >= 0]\n",
    "\n",
    "    # make sure the polygon is large enough\n",
    "    B *= 2 * 50\n",
    "\n",
    "    x, y = B * torch.cos(dtheta), B * torch.sin(dtheta)\n",
    "\n",
    "    # add an x-mirrored copy\n",
    "    x = torch.cat([-x, x])\n",
    "    y = torch.cat([y, y])\n",
    "    # results.append([x, y, theta_0])\n",
    "\n",
    "    # get true theta_0 from the angle between the two stations\n",
    "    theta_0 = torch.atan2(station_B[1] - station_A[1], station_B[0] - station_A[0])\n",
    "    # rotate x and y coordinates around center to align with the two stations\n",
    "    _x = x * np.cos(theta_0) - y * np.sin(theta_0)\n",
    "    _y = x * np.sin(theta_0) + y * np.cos(theta_0)\n",
    "\n",
    "    # shift to the center of the two stations\n",
    "    _x += (station_A[0] + station_B[0]) / 2\n",
    "    _y += (station_A[1] + station_B[1]) / 2\n",
    "\n",
    "    polygon = Polygon(zip(_x, _y))\n",
    "    mask = torch.tensor(\n",
    "        [polygon.contains(Point(p)) for p in stations_aux], dtype=torch.bool\n",
    "    )\n",
    "    masks_endfire.append(mask)\n",
    "\n",
    "    results.append([_x, _y, mask])\n",
    "\n",
    "masks_endfire = torch.stack(masks_endfire, dim=0)\n",
    "\n",
    "c2_correlations_endfire_filt = torch.zeros_like(c2_correlations_filt)\n",
    "for i, mask in enumerate(masks_endfire):\n",
    "    c2_correlations_endfire_filt[i] = c2_correlations_unstacked_filt_avg[i][mask].mean(\n",
    "        dim=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection based on azimuth\n",
    "# e.g., Zhang et al. 2020\n",
    "# thetas = torch.linspace(torch.pi / 16, torch.pi / 2, 6)\n",
    "# alphas = 1 - torch.cos(thetas)\n",
    "\n",
    "# Zhang et al. 2020 uses a fixed angle of 8 degrees, or alpha=0.01\n",
    "theta = np.radians(8)\n",
    "# Nouibat et al. 2022 uses a fixed angle of 20 degrees, or alpha=0.06\n",
    "theta = np.radians(20)\n",
    "\n",
    "alpha = 1 - np.cos(theta)\n",
    "# cos θ =1−α\n",
    "# alphas = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5]\n",
    "\n",
    "# stations_all = cart_coords[station_idxs_array]\n",
    "# stations_aux = cart_coords[station_idxs_auxilliary_subset]\n",
    "# station_A = cart_coords[master_idx][0]  # master station\n",
    "masks_angle = []\n",
    "for station_B in stations_all:\n",
    "    mask = []\n",
    "    for idx, source in enumerate(stations_aux):\n",
    "        d_a_b = torch.norm(station_A - station_B)\n",
    "        d_aux_a = torch.norm(source - station_A)\n",
    "        d_aux_b = torch.norm(source - station_B)\n",
    "        include_station = False\n",
    "        if abs(d_aux_a - d_aux_b) >= (1 - alpha) * d_a_b:\n",
    "            include_station = True\n",
    "        mask.append(include_station)\n",
    "    masks_angle.append(mask)\n",
    "\n",
    "masks_angle = torch.tensor(masks_angle)\n",
    "\n",
    "c2_correlations_angle_filt = torch.zeros_like(c2_correlations_filt)\n",
    "for i, mask in enumerate(masks_angle):\n",
    "    c2_correlations_angle_filt[i] = c2_correlations_unstacked_filt_avg[i][mask].mean(\n",
    "        dim=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aeb932",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MEASURE GROUP VELOCITY FOR EACH CORRELATION WAVEFIELD AT EVERY STATION\n",
    "\n",
    "distances_master = np.linalg.norm(array_stations - array_stations[master_idx], axis=1)\n",
    "lapse_times_c3 = torch.arange(\n",
    "    -window_length / 2, window_length / 2 + 1 / sampling_rate, 1 / sampling_rate\n",
    ")\n",
    "\n",
    "\n",
    "def get_group_velocity(correlations, lapse_times):\n",
    "    envelopes = np.abs(hilbert(correlations, axis=-1))\n",
    "    taper = torch.hann_window(envelopes.shape[-1])\n",
    "    envelopes *= taper.numpy()\n",
    "    picked_time_idx = np.argmax(envelopes, axis=-1)\n",
    "    picked_time = abs(lapse_times[picked_time_idx])\n",
    "    group_velocity = distances_master / picked_time\n",
    "    return group_velocity\n",
    "\n",
    "\n",
    "group_velocities_c2_angle = get_group_velocity(\n",
    "    c2_correlations_angle_filt, lapse_times_c3\n",
    ")\n",
    "group_velocity_anomalies_c2_angle = (\n",
    "    100 * (group_velocities_c2_angle - medium_velocity) / medium_velocity\n",
    ")\n",
    "\n",
    "group_velocities_c2_endfire = get_group_velocity(\n",
    "    c2_correlations_endfire_filt, lapse_times_c3\n",
    ")\n",
    "group_velocity_anomalies_c2_endfire = (\n",
    "    100 * (group_velocities_c2_endfire - medium_velocity) / medium_velocity\n",
    ")\n",
    "\n",
    "group_velocities_c2_all = get_group_velocity(c2_correlations_filt, lapse_times_c3)\n",
    "group_velocity_anomalies_c2_all = (\n",
    "    100 * (group_velocities_c2_all - medium_velocity) / medium_velocity\n",
    ")\n",
    "\n",
    "group_velocities_c1 = get_group_velocity(c1_correlations_filt, lapse_times_c1)\n",
    "group_velocity_anomalies_c1 = (\n",
    "    100 * (group_velocities_c1 - medium_velocity) / medium_velocity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e61bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(Path(\"../meta/colorblind_friendly.mplstyle\"))\n",
    "matplotlib.rcParams.update({\"font.family\": \"serif\", \"font.size\": 8})\n",
    "_cm = 1 / 2.54  # cm to inches\n",
    "\n",
    "lapse_times_c2 = torch.arange(\n",
    "    -window_length / 2, window_length / 2 + 1 / sampling_rate, 1 / sampling_rate\n",
    ")\n",
    "# coordinates\n",
    "idx_closest_to_0 = array_stations.abs().argmin()\n",
    "\n",
    "# select a box from which to compute amplitude peaks for normalisation\n",
    "array_selection = torch.where(\n",
    "    (array_stations[:, 0] >= -20)\n",
    "    & (array_stations[:, 0] <= -10)\n",
    "    & (array_stations[:, 1] >= 10)\n",
    "    & (array_stations[:, 1] <= 20)\n",
    ")\n",
    "# maybe [0] is needed here?\n",
    "time_idxs_c2 = torch.where((lapse_times_c2 >= 9.75) & (lapse_times_c2 < 10.25))[0]\n",
    "time_idxs_c1 = torch.where((lapse_times_c1 >= 9.75) & (lapse_times_c1 < 10.25))[0]\n",
    "print(c2_correlations_filt.shape)\n",
    "c2_all_peak = c2_correlations_filt[..., time_idxs_c2][array_selection].abs().max()\n",
    "c2_endfire_peak = (\n",
    "    c2_correlations_endfire_filt[..., time_idxs_c2][array_selection].abs().max()\n",
    ")\n",
    "c2_angle_peak = (\n",
    "    c2_correlations_angle_filt[..., time_idxs_c2][array_selection].abs().max()\n",
    ")\n",
    "c1_peak = c1_correlations_filt[..., time_idxs_c1][array_selection].abs().max()\n",
    "\n",
    "maxima_for_norm = [\n",
    "    c1_peak,\n",
    "    c2_all_peak,\n",
    "    c2_angle_peak,\n",
    "    c2_endfire_peak,\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(18 * _cm, 18 * _cm))\n",
    "times_to_plot = [-10, 0, 10] * 4\n",
    "labels = (\"a)\", \"b)\", \"c)\", \"e)\", \"f)\", \"g)\", \"i)\", \"j)\", \"k)\", \"m)\", \"n)\", \"o)\")\n",
    "for ax, time_to_plot, label in zip(axs[:, :-1].flatten(), times_to_plot, labels):\n",
    "    if ax in axs[0]:\n",
    "        focal_spot = c1_correlations_filt[\n",
    "            :, np.abs(lapse_times_c1 - time_to_plot).argmin()\n",
    "        ].clone()\n",
    "        max_norm = maxima_for_norm[0]\n",
    "    elif ax in axs[1]:\n",
    "        focal_spot = c2_correlations_filt[\n",
    "            :, np.abs(lapse_times_c2 - time_to_plot).argmin()\n",
    "        ].clone()\n",
    "        max_norm = maxima_for_norm[1]\n",
    "    elif ax in axs[2]:\n",
    "        focal_spot = c2_correlations_endfire_filt[\n",
    "            :, np.abs(lapse_times_c2 - time_to_plot).argmin()\n",
    "        ].clone()\n",
    "        max_norm = maxima_for_norm[2]\n",
    "    elif ax in axs[3]:\n",
    "        focal_spot = c2_correlations_angle_filt[\n",
    "            :, np.abs(lapse_times_c2 - time_to_plot).argmin()\n",
    "        ].clone()\n",
    "        max_norm = maxima_for_norm[3]\n",
    "\n",
    "    focal_spot /= max_norm\n",
    "\n",
    "    pcm_fs = ax.pcolormesh(\n",
    "        x_coords,\n",
    "        y_coords,\n",
    "        focal_spot.reshape(len(x_coords), len(y_coords)).T,\n",
    "        cmap=cm.broc,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(-35, 35)\n",
    "    ax.set_ylim(-35, 35)\n",
    "    ax.set_yticks([-30, 0, 30])\n",
    "    ax.set_xticks([-30, 0, 30])\n",
    "    ax.set_xticklabels([-30, 0, 30], fontsize=10)\n",
    "    ax.set_yticklabels([-30, 0, 30], fontsize=10)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(f\"{label}\", loc=\"left\", fontsize=10, pad=4)\n",
    "\n",
    "    if ax in axs[0]:\n",
    "        clbl = r\"$C_1$\"\n",
    "        ax.set_title(rf\"$\\tau$ = {time_to_plot} s\", fontsize=10, pad=4)\n",
    "    elif ax in axs[1]:\n",
    "        clbl = r\"$C^{all}_2$\"\n",
    "    elif ax in axs[2]:\n",
    "        clbl = r\"$C^{endfire}_2$\"\n",
    "    elif ax in axs[3]:\n",
    "        clbl = r\"$C^{angle}_2$\"\n",
    "\n",
    "    t = ax.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        clbl,\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"none\", alpha=0.7),\n",
    "    )\n",
    "    ax.scatter([0], [0], marker=\"v\", ec=\"k\", s=100, lw=1, c=\"#FFA90E\")\n",
    "\n",
    "for ax in axs[-1, :].flat:\n",
    "    ax.set_xlabel(\"Distance [km]\", labelpad=0, fontsize=10)\n",
    "for ax in axs[:, 0]:\n",
    "    ax.set_ylabel(\"Distance [km]\", labelpad=0, fontsize=10)\n",
    "\n",
    "for ax in axs[:3, :-1].flat:\n",
    "    ax.set_xticklabels([])\n",
    "for ax in axs[:3, 1:-1].flat:\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "### group vel errors\n",
    "\n",
    "for i, (group_velocity_anomalies, lbl) in enumerate(\n",
    "    zip(\n",
    "        [\n",
    "            group_velocity_anomalies_c1,\n",
    "            group_velocity_anomalies_c2_all,\n",
    "            group_velocity_anomalies_c2_endfire,\n",
    "            group_velocity_anomalies_c2_angle,\n",
    "        ],\n",
    "        [\"d)\", \"h)\", \"l)\", \"p)\"],\n",
    "    )\n",
    "):\n",
    "    ax = axs[i, -1]\n",
    "    pcm = ax.pcolormesh(\n",
    "        x_coords,\n",
    "        y_coords,\n",
    "        group_velocity_anomalies.reshape(x_coords.shape[0], y_coords.shape[0]).T,\n",
    "        vmin=-15,\n",
    "        vmax=15,\n",
    "        # cmap=cm.cork,\n",
    "        cmap=cm.bam_r,\n",
    "    )\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlim(-35, 35)\n",
    "    ax.set_ylim(-35, 35)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_yticks([-30, 0, 30])\n",
    "    ax.set_xticks([-30, 0, 30])\n",
    "    ax.set_xticklabels([-30, 0, 30], fontsize=10)\n",
    "    ax.set_yticklabels([-30, 0, 30], fontsize=10)\n",
    "    ax.set_title(lbl, loc=\"left\", fontsize=10, pad=4)\n",
    "\n",
    "    ax.scatter([0], [0], marker=\"v\", ec=\"k\", s=100, lw=1, c=\"#FFA90E\")\n",
    "\n",
    "# add colorbar to the last column\n",
    "x0, y0, w0, h0 = axs[-2, -1].get_position().bounds\n",
    "x1, y1, w1, h1 = axs[-1, -1].get_position().bounds\n",
    "vertical_space_between_both = y0 - (y1 + h1)\n",
    "x0, y0, w0, h0 = axs[0, -1].get_position().bounds\n",
    "cbar_ax = fig.add_axes([x0 + w0 + 0.03, y0 + h0 / 2 - 0.02, w0 / 1.5, 0.01])\n",
    "cbar = fig.colorbar(\n",
    "    pcm,\n",
    "    cax=cbar_ax,\n",
    "    orientation=\"horizontal\",\n",
    "    extend=\"both\",\n",
    "    ticks=[-15, 0, 15],\n",
    ")\n",
    "cbar.ax.tick_params(labelsize=8)\n",
    "cbar.ax.set_xticklabels([\"-15\", \"0\", \"15\"], fontsize=8)\n",
    "cbar.ax.set_xlabel(\"Group velocity\\nerror [%]\", fontsize=8, labelpad=4)\n",
    "\n",
    "# another colorbar for pcm_fs\n",
    "cbar_ax2 = fig.add_axes([x0 + w0 + 0.03, y0 + h0 - 0.02, w0 / 1.5, 0.01])\n",
    "cbar2 = fig.colorbar(\n",
    "    pcm_fs,\n",
    "    cax=cbar_ax2,\n",
    "    orientation=\"horizontal\",\n",
    "    extend=\"both\",\n",
    "    ticks=[-1, 0, 1],\n",
    ")\n",
    "cbar2.ax.tick_params(labelsize=8)\n",
    "cbar2.ax.set_xticklabels([\"-1\", \"0\", \"1\"], fontsize=8)\n",
    "cbar2.ax.set_xlabel(\"Simulated\\namplitudes\", fontsize=8, labelpad=4)\n",
    "\n",
    "# cleanup\n",
    "for ax in axs[:, -1]:\n",
    "    ax.set_yticklabels([])\n",
    "    if ax in axs[:-1, -1]:\n",
    "        ax.set_xticklabels([])\n",
    "    if ax == axs[-1, -1]:\n",
    "        ax.set_xlabel(\"Distance [km]\", labelpad=0, fontsize=10)\n",
    "for ax in axs[-1, 1:]:\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "fig.savefig(\"../figures/figure9.png\", dpi=300, bbox_inches=\"tight\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
